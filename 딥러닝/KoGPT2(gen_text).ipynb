{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6114dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoGPT2의 TFGPT2LMHeadModel 모델을 이용한 언어 생성\n",
    "# 참고 : https://github.com/SKT-AI/KoGPT2\n",
    "# TFGPT2LMHeadModel : The GPT2 Model transformer with a language modeling head on top \n",
    "#                     (linear layer with weights tied to the input embeddings).\n",
    "!pip install --upgrade mxnet>=1.6.0\n",
    "!pip install gluonnlp\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install wget\n",
    "\n",
    "import gluonnlp as nlp\n",
    "from gluonnlp.data import SentencepieceTokenizer, SentencepieceDetokenizer\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "wget.download('https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/releases/download/v1.0/gpt_ckpt.zip')\n",
    "\n",
    "with zipfile.ZipFile('gpt_ckpt.zip') as z:\n",
    "    z.extractall()\n",
    "\n",
    "MODEL_PATH = '/content/gpt_ckpt'\n",
    "TOKENIZER_PATH = '/content/gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
    "\n",
    "\n",
    "# 참고 : https://nlp.gluon.ai/api/modules/data.html\n",
    "#        https://opensourcelibs.com/lib/kogpt2#mxnet-gluon\n",
    "#        https://github.com/SKT-AI/KoGPT2#user-contributed-examples\n",
    "# alpha = 1.0 (default)으로 설정하면, '안녕 하세요' --> ['▁', '안', '녕', '하', '세', '요']로 분해됨.\n",
    "# alpha = 0으로 설정하면 ['▁안녕', '▁하세요']로 분해됨. 한글은 alpha = 0으로 설정함.\n",
    "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH, num_best=0, alpha=0)\n",
    "detokenizer = SentencepieceDetokenizer(TOKENIZER_PATH)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
    "                                               mask_token = None,\n",
    "                                               sep_token = None,\n",
    "                                               cls_token = None,\n",
    "                                               unknown_token = '<unk>',\n",
    "                                               padding_token = '<pad>',\n",
    "                                               bos_token = '<s>',\n",
    "                                               eos_token = '</s>')\n",
    "# vocab --> Vocab(size=50000, unk=\"<unk>\", reserved=\"['<pad>', '<s>', '</s>']\")\n",
    "\n",
    "# tokenizer 연습\n",
    "toked = tokenizer('안녕 하세요')\n",
    "print(toked)\n",
    "\n",
    "toked_idx = vocab(toked)\n",
    "print(toked_idx)\n",
    "\n",
    "toked = vocab.to_tokens(toked_idx)\n",
    "print(toked)\n",
    "\n",
    "detoked = detokenizer(toked)\n",
    "print(detoked)\n",
    "\n",
    "''.join(toked).replace('▁', ' ')[1:]\n",
    "\n",
    "print(len(vocab))\n",
    "print(vocab.padding_token, ':', vocab[vocab.padding_token])\n",
    "print(vocab.bos_token, ': ', vocab[vocab.bos_token])\n",
    "print(vocab.eos_token, ': ', vocab[vocab.eos_token])\n",
    "print(vocab.unknown_token, ': ', vocab[vocab.unknown_token])\n",
    "\n",
    "# vocabulry = vocab.token_to_idx\n",
    "word2idx = {k:v for k, v in vocab.token_to_idx.items()}\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "idx2word[5000]\n",
    "\n",
    "print(vocab.token_to_idx)\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained(MODEL_PATH)\n",
    "model.summary()\n",
    "\n",
    "# 모델의 seed 입력 문장 생성\n",
    "tok = tokenizer('이때')   # tok = ['▁이때']\n",
    "tok_idx = [vocab[vocab.bos_token]] + vocab[tok]     # tok_idx = [0, 4499]\n",
    "input_ids = tf.convert_to_tensor(tok_idx)[None, :]  # 텐서로 변환\n",
    "\n",
    "input_ids\n",
    "\n",
    "# 모델의 출력\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "output\n",
    "\n",
    "# 모델의 출력을 문자열로 변환\n",
    "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
    "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
    "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
    "print(out_text)\n",
    "\n",
    "# Beam search\n",
    "output = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
    "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
    "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
    "print(out_text)\n",
    "\n",
    "# 연속된 단어가 나오는 것을 방지함. no_repeat_ngram_size = 2\n",
    "output = model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "\n",
    "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
    "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
    "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
    "print(out_text)\n",
    "\n",
    "# top_k sampling. 확률이 높은 상위 k개에서 랜덤 샘플링.\n",
    "output = model.generate(input_ids, max_length=50, do_sample = True, top_k=100, temperature=0.8)\n",
    "\n",
    "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
    "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
    "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
    "print(out_text)\n",
    "\n",
    "# top_p sampling. 확률이 높은 순서로 누적 확률이 top_p인 단어들을 랜덤 샘플링.\n",
    "output = model.generate(input_ids, max_length=50, do_sample = True, top_p=0.9, temperature=0.8)\n",
    "\n",
    "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
    "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
    "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
    "print(out_text)\n",
    "\n",
    "# top_k & top_p sampling\n",
    "output = model.generate(input_ids, max_length=50, do_sample = True, top_k=100, top_p=0.9, temperature=0.8)\n",
    "\n",
    "out_tok_idx = output.numpy().tolist()[0]   # output token 인덱스\n",
    "out_tok = vocab.to_tokens(out_tok_idx)     # token 인덱스를 token 문자로 변환\n",
    "out_text = detokenizer(out_tok)            # 출력 문자열로 decode\n",
    "print(out_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
