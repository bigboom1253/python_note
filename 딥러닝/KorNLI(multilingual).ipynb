{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f34ac12",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert because of the following error (look up to see its traceback):\ninvalid syntax (__init__.py, line 172)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2776\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2777\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2778\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\transformers\\models\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m from . import (\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0malbert\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\transformers\\models\\mt5\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_sentencepiece_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenization_t5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msentencepiece\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSyntaxError\u001b[0m: invalid syntax (__init__.py, line 172)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1204/2741266903.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTFBertModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2765\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2766\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2767\u001b[1;33m             \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2768\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2769\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2777\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2778\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2779\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m   2780\u001b[0m                 \u001b[1;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its traceback):\\n{e}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2781\u001b[0m             ) from e\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert because of the following error (look up to see its traceback):\ninvalid syntax (__init__.py, line 172)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "# https://github.com/kakaobrain/KorNLUDatasets\n",
    "DATA_PATH = 'C:\\\\Users\\\\배진우\\\\Documents\\\\multiCampus_TA\\\\python_data\\\\'\n",
    "df = pd.read_csv(DATA_PATH + 'multinli.train.koNLI.tsv', delimiter = '\\t', quoting = 3)\n",
    "df = df.dropna()\n",
    "\n",
    "# gold_label을 수치화한다. [contradiction (모순 관계) = 0, entailment (얽힘) = 1, neutral (중립) = 2]\n",
    "df['label'] = LabelEncoder().fit_transform(df['gold_label'])\n",
    "\n",
    "df = df[:100]   # 시험용으로 조금만 사용\n",
    "df.head()\n",
    "\n",
    "# DATA_PATH 아래에 모델 파라메터를 저장할 파일을 설정한다.\n",
    "ckpt_path = DATA_PATH + 'konli_weights.h5'\n",
    "\n",
    "# Create a callback that saves the model's weights, every epochs\n",
    "cp_callback = ModelCheckpoint(filepath=ckpt_path, \n",
    "                              save_weights_only=True, \n",
    "                              verbose=1,\n",
    "                              save_freq=1)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt', do_lower_case=False)\n",
    "word2idx = tokenizer.vocab\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "\n",
    "MAX_LEN = 24 * 2\n",
    "\n",
    "# Bert Tokenizer\n",
    "def bert_tokenizer_v2(sent1, sent2):\n",
    "    \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent1,\n",
    "        text_pair = sent2,\n",
    "        add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
    "        max_length = MAX_LEN,           # Pad & truncate all sentences.\n",
    "        padding = 'max_length',\n",
    "        # pad_to_max_length = True,\n",
    "        return_attention_mask = True,   # Construct attn. masks.\n",
    "        truncation = True\n",
    "    )\n",
    "    \n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask'] # And its attention mask (simply differentiates padding from non-padding).\n",
    "    token_type_id = encoded_dict['token_type_ids']  # differentiate two sentences\n",
    "    \n",
    "    return input_id, attention_mask, token_type_id\n",
    "\n",
    "text1 = df['sentence1'][0]\n",
    "text2 = df['sentence2'][0]\n",
    "\n",
    "id, mask, typ = bert_tokenizer_v2(text1, text2)\n",
    "print(text1)\n",
    "print(text2)\n",
    "\n",
    "print(id)\n",
    "print(mask)\n",
    "print(typ)\n",
    "\n",
    "text_1 = [idx2word[x] for x in id]\n",
    "print(text_1)\n",
    "\n",
    "# 문장 복원\n",
    "print((' '.join(text_1)).replace(' ##', ''))  # 원리\n",
    "print(tokenizer.decode(id))\n",
    "\n",
    "print('토큰 길이 =', len(id))\n",
    "\n",
    "def build_data(sent1, sent2):\n",
    "    x_ids = []\n",
    "    x_msk = []\n",
    "    x_typ = []\n",
    "\n",
    "    for s1, s2 in tqdm(zip(sent1, sent2), total=len(sent1)):\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer_v2(s1, s2)\n",
    "\n",
    "        x_ids.append(input_id)\n",
    "        x_msk.append(attention_mask)\n",
    "        x_typ.append(token_type_id)\n",
    "    \n",
    "    x_ids = np.array(x_ids, dtype=int)\n",
    "    x_msk = np.array(x_msk, dtype=int)\n",
    "    x_typ = np.array(x_typ, dtype=int)\n",
    "    # pdb.set_trace()\n",
    "\n",
    "    return x_ids, x_msk, x_typ\n",
    "\n",
    "sent1 = df['sentence1']\n",
    "sent2 = df['sentence2']\n",
    "y_label = df['label']\n",
    "\n",
    "x_train1, x_test1, x_train2, x_test2, y_train, y_test = train_test_split(sent1, sent2, y_label, test_size=0.1, random_state = 0)\n",
    "\n",
    "x_train_ids, x_train_msk, x_train_typ = build_data(x_train1, x_train2)\n",
    "x_test_ids, x_test_msk, x_test_typ = build_data(x_test1, x_test2)\n",
    "\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "y_test = np.array(y_test).reshape(-1, 1)\n",
    "\n",
    "x_train_ids.shape, y_train.shape, x_test_ids.shape, y_test.shape\n",
    "\n",
    "x_train_ids[1]\n",
    "\n",
    "x_train_msk[1]\n",
    "\n",
    "x_train_typ[1]\n",
    "\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt')\n",
    "bert_model.summary() # bert_model을 확인한다. trainable params = 177,854,978\n",
    "\n",
    "# BERT 입력\n",
    "# ---------\n",
    "x_input_ids = Input(batch_shape = (None, MAX_LEN), dtype = tf.int32)\n",
    "x_input_msk = Input(batch_shape = (None, MAX_LEN), dtype = tf.int32)\n",
    "x_input_typ = Input(batch_shape = (None, MAX_LEN), dtype = tf.int32)\n",
    "\n",
    "# BERT 출력\n",
    "# ---------\n",
    "output_bert = bert_model([x_input_ids, x_input_msk, x_input_typ])[1]\n",
    "\n",
    "# Downstream task : 자언어 추론 (NLI)\n",
    "# ----------------------------------\n",
    "y_output = Dense(3, activation = 'softmax')(output_bert)\n",
    "model = Model([x_input_ids, x_input_msk, x_input_typ], y_output)\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = Adam(learning_rate = 0.001))\n",
    "model.summary()\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    model.load_weights(ckpt_path)\n",
    "    print('학습된 weight가 적용됐습니다.')\n",
    "\n",
    "x_train = [x_train_ids, x_train_msk, x_train_typ]\n",
    "x_test = [x_test_ids, x_test_msk, x_test_typ]\n",
    "hist = model.fit(x_train, y_train, \n",
    "                 validation_data = (x_test, y_test), \n",
    "                 epochs=10, \n",
    "                 batch_size=1024,\n",
    "                 callbacks=[cp_callback])\n",
    "\n",
    "# Loss history를 그린다\n",
    "plt.plot(hist.history['loss'], label='Train loss')\n",
    "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "# 시험 데이터로 학습 성능을 평가한다\n",
    "pred = model.predict(x_test)\n",
    "y_pred = np.argmax(pred, axis=1).reshape(-1, 1)\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(\"\\nAccuracy = %.2f %s\" % (accuracy * 100, '%'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ada15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
