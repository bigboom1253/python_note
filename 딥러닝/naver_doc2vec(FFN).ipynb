{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de13c0a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"C:\\Users\\배진우\\Documents\\multiCampus_TA\\python_data\\naver_movie\\naver_data.txt\": Illegal byte sequence Error #42",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23440/1215288266.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemplates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mspm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentencePieceTrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[0msp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sentencepiece\\__init__.py\u001b[0m in \u001b[0;36mTrain\u001b[1;34m(arg, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m       \u001b[1;34m\"\"\"Train Sentencepiece model. Accept both kwargs and legacy string arg.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSentencePieceTrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_TrainFromString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sentencepiece\\__init__.py\u001b[0m in \u001b[0;36m_TrainFromString\u001b[1;34m(arg)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_TrainFromString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentencePieceTrainer__TrainFromString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Not found: \"C:\\Users\\배진우\\Documents\\multiCampus_TA\\python_data\\naver_movie\\naver_data.txt\": Illegal byte sequence Error #42"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %cd '/content/drive/My Drive/Colab Notebooks'\n",
    "DATA_PATH = 'C:\\\\Users\\\\배진우\\\\Documents\\\\multiCampus_TA\\\\python_data\\\\naver_movie\\\\'\n",
    "\n",
    "train_data = pd.read_csv(DATA_PATH + 'ratings_train.txt', sep='\\t')\n",
    "test_data = pd.read_csv(DATA_PATH + 'ratings_test.txt', sep='\\t')\n",
    "train_data = train_data.dropna()\n",
    "test_data = test_data.dropna()\n",
    "\n",
    "train_data.head()\n",
    "\n",
    "# 기호, 숫자, 영어 등은 제외하고 한글만 사용한다.\n",
    "train_list = [re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]\", \"\", x) for x in train_data['document']]\n",
    "test_list = [re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]\", \"\", x) for x in test_data['document']]\n",
    "\n",
    "n_train = len(train_list)\n",
    "n_test = len(test_list)\n",
    "\n",
    "# Sentencepice용 사전을 만들기 위해 train_list, test_list를 저장해 둔다.\n",
    "data_file = DATA_PATH + \"naver_data.txt\"\n",
    "with open(data_file, 'w', encoding='utf-8') as f:\n",
    "    for sent in train_list + test_list:\n",
    "        f.write(sent + '\\n')\n",
    "        \n",
    "# Google의 Sentencepiece를 이용해서 vocabulary를 생성한다.\n",
    "templates= \"--input={} \\\n",
    "            --pad_id=0 --pad_piece=<PAD>\\\n",
    "            --unk_id=1 --unk_piece=<UNK>\\\n",
    "            --bos_id=2 --bos_piece=<BOS>\\\n",
    "            --eos_id=3 --eos_piece=<EOS>\\\n",
    "            --model_prefix={} \\\n",
    "            --vocab_size={}\"\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "model_prefix = \"data/naver_model\"\n",
    "params = templates.format(data_file, model_prefix, VOCAB_SIZE)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(params)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(model_prefix + '.model')\n",
    "\n",
    "with open(model_prefix + '.vocab', encoding='utf-8') as f:\n",
    "    vocab = [doc.strip().split('\\t') for doc in f]\n",
    "\n",
    "word2idx = {k:v for v, [k, _] in enumerate(vocab)}\n",
    "idx2word = {v:k for v, [k, _] in enumerate(vocab)}\n",
    "\n",
    "# 문서를 서브 워드로 분해한다.\n",
    "train_subword = [sp.encode_as_pieces(x) for x in train_list]\n",
    "test_subword = [sp.encode_as_pieces(x) for x in test_list]\n",
    "\n",
    "train_subword[0]\n",
    "\n",
    "# test data까지 학습에 이용하는 것은 데이터 분석의 정석은 아니다.\n",
    "# 정석대로 하려면 train data로만 doc2vec을 학습하고, test data는 inference stage로\n",
    "# 추정해야 한다. 그러나 kaggle 형태의 공모전에서는 test data의 feature 부분을 이용하는 \n",
    "# 것은 허용된다. 데이터를 표준화할 때 test data의 feature 부분을 이용하는 것과 동일하다.\n",
    "# 이 코드는 doc2vec의 기능 시험을 위한 것으로 test data까지 포함해서 doc2vec을 학습시켜 본다.\n",
    "tag_sent = [TaggedDocument(x, [i]) for i, x in enumerate(train_subword + test_subword)]\n",
    "tag_sent[0]\n",
    "\n",
    "model = Doc2Vec(tag_sent, vector_size=32, window=5, min_count=1)\n",
    "\n",
    "d_vector = [model.docvecs[tags[0]] for _, tags in tag_sent]\n",
    "\n",
    "x_train = np.array(d_vector[:n_train])\n",
    "x_test = np.array(d_vector[n_train:])\n",
    "x_train.shape, x_test.shape\n",
    "\n",
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])\n",
    "\n",
    "y_train.shape, y_test.shape\n",
    "\n",
    "# 학습 데이터를 저장해 둔다.\n",
    "with open('data/naver_doc2vec.pkl', 'wb') as f:\n",
    "    pickle.dump([x_train, x_test, y_train, y_test], f, pickle.DEFAULT_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a961da",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\배진우\\\\Documents\\\\multiCampus_TA\\\\python_data\\\\naver_movie\\\\naver_doc2vec.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23440/2978555124.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# 학습 데이터를 읽어온다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'naver_doc2vec.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\배진우\\\\Documents\\\\multiCampus_TA\\\\python_data\\\\naver_movie\\\\naver_doc2vec.pkl'"
     ]
    }
   ],
   "source": [
    "# Doc2Vec을 이용한 네이버 영화 감성분석\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "DATA_PATH = 'C:\\\\Users\\\\배진우\\\\Documents\\\\multiCampus_TA\\\\python_data\\\\naver_movie\\\\'\n",
    "\n",
    "\n",
    "# 학습 데이터를 읽어온다.\n",
    "with open(DATA_PATH + 'naver_doc2vec.pkl', 'rb') as f:\n",
    "    x_train, x_test, y_train, y_test = pickle.load(f)\n",
    "\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "y_test = np.array(y_test).reshape(-1, 1)\n",
    "\n",
    "# FFN 모델을 빌드한다.\n",
    "x_input = Input(batch_shape=(None, x_train.shape[1]))\n",
    "h_layer = Dense(128, activation='relu')(x_input)\n",
    "h_layer = Dropout(rate=0.5)(h_layer)\n",
    "h_layer = Dense(128, activation='relu')(h_layer)\n",
    "h_layer = Dropout(rate=0.5)(h_layer)\n",
    "y_output = Dense(1, activation='sigmoid')(h_layer)\n",
    "\n",
    "model = Model(x_input, y_output)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
    "model.summary()\n",
    "\n",
    "# 학습\n",
    "hist = model.fit(x_train, y_train, \n",
    "                 validation_data = (x_test, y_test), \n",
    "                 batch_size = 2048, \n",
    "                 epochs = 100)\n",
    "\n",
    "# Loss history를 그린다\n",
    "plt.plot(hist.history['loss'], label='Train loss')\n",
    "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "# 시험 데이터로 학습 성능을 평가한다\n",
    "pred = model.predict(x_test)\n",
    "y_pred = np.where(pred > 0.5, 1, 0)\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(\"\\nAccuracy = %.2f %s\" % (accuracy * 100, '%'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd15bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
