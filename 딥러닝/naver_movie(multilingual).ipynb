{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee28f597",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5940/2629216349.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTFBertModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt', do_lower_case=False)\n",
    "word2idx = tokenizer.vocab\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "\n",
    "enc = tokenizer.encode('I love you'.split())\n",
    "print(enc)\n",
    "print([idx2word[x] for x in enc])\n",
    "\n",
    "dec = tokenizer.decode(enc)\n",
    "print(dec)\n",
    "\n",
    "DATA_PATH = 'C:\\\\Users\\\\배진우\\\\Documents\\\\multiCampus_TA\\\\python_data\\\\naver_movie\\\\'\n",
    "df = pd.read_csv(DATA_PATH + 'ratings.txt', header=0, delimiter='\\t', quoting=3)\n",
    "df = df.dropna()\n",
    "\n",
    "# 간단한 전처리. 한글이 아닌 숫자, 영문자, 기호 등은 공백문자로 치환.\n",
    "df['document'] = df['document'].apply(lambda x: re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]\", \" \", x))\n",
    "df.drop('id', axis = 1, inplace = True)\n",
    "\n",
    "document = list(df['document'])\n",
    "label = list(df['label'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(document, label, test_size=0.2)\n",
    "\n",
    "MAX_LEN = 60\n",
    "\n",
    "# Bert Tokenizer\n",
    "def bert_tokenizer(sent):\n",
    "    \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent,\n",
    "        add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
    "        max_length = MAX_LEN,           # Pad & truncate all sentences.\n",
    "        padding = 'max_length',\n",
    "        # pad_to_max_length = True,\n",
    "        return_attention_mask = True,   # Construct attn. masks.\n",
    "        truncation = True\n",
    "    )\n",
    "    \n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask'] # And its attention mask (simply differentiates padding from non-padding).\n",
    "    token_type_id = encoded_dict['token_type_ids']  # differentiate two sentences\n",
    "    \n",
    "    return input_id, attention_mask, token_type_id\n",
    "\n",
    "text = document[1]\n",
    "id, mask, typ = bert_tokenizer(text)\n",
    "print(text)\n",
    "print(id)\n",
    "print(mask)\n",
    "print(typ)\n",
    "\n",
    "text_1 = [idx2word[x] for x in id]\n",
    "print(text_1)\n",
    "\n",
    "# 문장 복원\n",
    "print((' '.join(text_1)).replace(' ##', ''))  # 원리\n",
    "print(tokenizer.decode(id))\n",
    "\n",
    "print('토큰 길이 =', len(id))\n",
    "\n",
    "def build_data(doc):\n",
    "    x_ids = []\n",
    "    x_msk = []\n",
    "    x_typ = []\n",
    "\n",
    "    for sent in tqdm(doc, total=len(doc)):\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(sent)\n",
    "        x_ids.append(input_id)\n",
    "        x_msk.append(attention_mask)\n",
    "        x_typ.append(token_type_id)\n",
    "\n",
    "    x_ids = np.array(x_ids, dtype=int)\n",
    "    x_msk = np.array(x_msk, dtype=int)\n",
    "    x_typ = np.array(x_typ, dtype=int)\n",
    "\n",
    "    return x_ids, x_msk, x_typ\n",
    "\n",
    "x_train_ids, x_train_msk, x_train_typ = build_data(x_train)\n",
    "x_test_ids, x_test_msk, x_test_typ = build_data(x_test)\n",
    "\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "y_test = np.array(y_test).reshape(-1, 1)\n",
    "\n",
    "x_train_ids.shape, y_train.shape\n",
    "x_test_ids.shape, y_test.shape\n",
    "\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt')\n",
    "bert_model.summary() # bert_model을 확인한다. trainable params = 177,854,978\n",
    "\n",
    "# 시간이 오래 걸리므로 bert_model의 fine-tuning을 잠시 막아 놓자.\n",
    "# downstream task 학습을 위해 bert_model도 fine-tuning해야 한다.\n",
    "bert_model.trainable = False\n",
    "bert_model.summary() # bert_model을 다시 확인한다. trainable params = 0\n",
    "\n",
    "# BERT 입력\n",
    "# ---------\n",
    "x_input_ids = Input(batch_shape = (None, MAX_LEN), dtype = tf.int32)\n",
    "x_input_msk = Input(batch_shape = (None, MAX_LEN), dtype = tf.int32)\n",
    "x_input_typ = Input(batch_shape = (None, MAX_LEN), dtype = tf.int32)\n",
    "\n",
    "# BERT 출력\n",
    "# [0]: (None, 60, 768) - sequence_output, [1]: (None, 768) - pooled_output\n",
    "# ------------------------------------------------------------------------\n",
    "output_bert = bert_model([x_input_ids, x_input_msk, x_input_typ])[0]\n",
    "\n",
    "# Downstream task : 네이버 영화 감성분석\n",
    "# -------------------------------------\n",
    "y_output = Dense(1, activation = 'sigmoid')(output_bert)\n",
    "model = Model([x_input_ids, x_input_msk, x_input_typ], y_output)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.01))\n",
    "model.summary()\n",
    "\n",
    "x_train = [x_train_ids, x_train_msk, x_train_typ]\n",
    "x_test = [x_test_ids, x_test_msk, x_test_typ]\n",
    "hist = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs=3, batch_size=1024)\n",
    "\n",
    "# Epoch 1/3\n",
    "# 157/157 [==============================] - ETA: 0s - loss: 0.6245WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
    "# WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
    "# 157/157 [==============================] - 936s 6s/step - loss: 0.6245 - val_loss: 0.6070\n",
    "# Epoch 2/3\n",
    "# 157/157 [==============================] - 926s 6s/step - loss: 0.6184 - val_loss: 0.6023\n",
    "# Epoch 3/3\n",
    "# 157/157 [==============================] - 903s 6s/step - loss: 0.6134 - val_loss: 0.5973\n",
    "\n",
    "# Loss history를 그린다\n",
    "plt.plot(hist.history['loss'], label='Train loss')\n",
    "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "# 시험 데이터로 학습 성능을 평가한다\n",
    "pred = model.predict(x_test)\n",
    "y_pred = np.where(pred > 0.5, 1, 0)\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(\"\\nAccuracy = %.2f %s\" % (accuracy * 100, '%'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc207e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
