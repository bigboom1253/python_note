{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97bf97b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718c63a77c6943ba8928fcf530d2b7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d384e0b252b841f59565896b846d8839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([464,  21, 266, 665,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DATA_PATH = 'C:\\\\Users\\\\배진우\\\\Documents\\\\multiCampus_TA\\\\python_data\\\\naver_movie\\\\'\n",
    "\n",
    "train_data = pd.read_csv(DATA_PATH + 'ratings_train.txt', sep='\\t')\n",
    "test_data = pd.read_csv(DATA_PATH + 'ratings_test.txt', sep='\\t')\n",
    "train_data = train_data.dropna()\n",
    "test_data = test_data.dropna()\n",
    "\n",
    "train_data.head()\n",
    "\n",
    "# 전처리 작업\n",
    "stop_words = ['은', '는', '이', '가', '하', '아', '것', '들',\n",
    "              '의', '있', '되', '수', '보', '주', '등', '한']\n",
    "okt = Okt()\n",
    "clean_train = []\n",
    "\n",
    "def preprocessing(review, okt, remove_stopwords = False, stop_words = []):\n",
    "    review_text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]\", \"\", review)\n",
    "    word_review = okt.morphs(review_text, stem=True)\n",
    "   \n",
    "    if remove_stopwords:\n",
    "        word_review = [token for token in word_review if not token in stop_words]\n",
    "        \n",
    "    return word_review\n",
    "\n",
    "def cleaning(df):\n",
    "    clean_data = []\n",
    "    for i, review in tqdm(enumerate(df['document']), total = len(df['document'])):\n",
    "        # 비어있는 데이터에서 멈추지 않도록 string인 경우만 진행\n",
    "        if type(review) == str:\n",
    "            p = preprocessing(review, okt, remove_stopwords = True, stop_words=stop_words)\n",
    "            clean_data.append(p)\n",
    "        else:\n",
    "            clean_data.append([])  # string이 아니면 비어있는 값 추가\n",
    "            print(i, review)\n",
    "    \n",
    "    return clean_data\n",
    "\n",
    "clean_train = cleaning(train_data)\n",
    "clean_test = cleaning(test_data)\n",
    "\n",
    "# 전체 vocabulary size = 43,756. 이중 빈도수 상위 2만개만 사용함.\n",
    "VOCAB_SIZE = 20000\n",
    "tokenizer = Tokenizer(num_words = VOCAB_SIZE, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(clean_train)  # train 데이터만으로 사전을 생성함.\n",
    "\n",
    "# 각 단어를 사전의 인덱스로 표시\n",
    "train_seq = tokenizer.texts_to_sequences(clean_train)\n",
    "test_seq = tokenizer.texts_to_sequences(clean_test)\n",
    "\n",
    "# 2만개 짜리 사전을 생성한다.\n",
    "word2idx = {k:v for k, v in tokenizer.word_index.items() if v < VOCAB_SIZE}\n",
    "word2idx['<PAD>'] = 0\n",
    "\n",
    "MAX_SEQ_LEN = 8  # 문장 최대 길이\n",
    "x_train = pad_sequences(train_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
    "x_test = pad_sequences(test_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
    "\n",
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])\n",
    "\n",
    "# 학습 데이터를 저장해 둔다.\n",
    "with open(DATA_PATH + 'naver_movie.pkl', 'wb') as f:\n",
    "    pickle.dump([x_train, x_test, y_train, y_test, word2idx], f, pickle.DEFAULT_PROTOCOL)\n",
    "\n",
    "x_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dccf85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
