{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba63107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# 데이터 파일을 읽어온다.\n",
    "data_df = pd.read_csv('data/ChatBotData.csv', header=0)\n",
    "question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "\n",
    "# 특수 문자를 제거한다.\n",
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "question = [re.sub(FILTERS, \"\", s) for s in question]\n",
    "answer = [re.sub(FILTERS, \"\", s) for s in answer]\n",
    "\n",
    "data_df.head()\n",
    "\n",
    "# # 학습 데이터와 시험 데이터를 분리한다.\n",
    "# que_train, que_test, ans_train, ans_test = train_test_split(question, answer, test_size=0.1, random_state=0)\n",
    "\n",
    "# que_train[0], ans_train[0]\n",
    "\n",
    "question[0], answer[0]\n",
    "\n",
    "# Sentencepice용 사전을 만들기 위해 que_train + que_test를 저장해 둔다.\n",
    "data_file = \"data/chatbot_data.txt\"\n",
    "with open(data_file, 'w', encoding='utf-8') as f:\n",
    "    for sent in question + answer:\n",
    "        f.write(sent + '\\n')\n",
    "        \n",
    "# Google의 Sentencepiece를 이용해서 vocabulary를 생성한다.\n",
    "# -----------------------------------------------------\n",
    "templates= \"--input={} \\\n",
    "            --pad_id=0 --pad_piece=<PAD>\\\n",
    "            --unk_id=1 --unk_piece=<UNK>\\\n",
    "            --bos_id=2 --bos_piece=<BOS>\\\n",
    "            --eos_id=3 --eos_piece=<EOS>\\\n",
    "            --model_prefix={} \\\n",
    "            --vocab_size={}\"\n",
    "\n",
    "VOCAB_SIZE = 9000\n",
    "model_prefix = \"data/chatbot_model\"\n",
    "params = templates.format(data_file, model_prefix, VOCAB_SIZE)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(params)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(model_prefix + '.model')\n",
    "\n",
    "with open(model_prefix + '.vocab', encoding='utf-8') as f:\n",
    "    vocab = [doc.strip().split('\\t') for doc in f]\n",
    "\n",
    "word2idx = {k:v for v, [k, _] in enumerate(vocab)}\n",
    "idx2word = {v:k for v, [k, _] in enumerate(vocab)}\n",
    "\n",
    "# 학습 데이터를 생성한다. (인코더 입력용, 디코더 입력용, 디코더 출력용)\n",
    "MAX_LEN = 15\n",
    "enc_input = []\n",
    "dec_input = []\n",
    "dec_output = []\n",
    "\n",
    "for Q, A in zip(question, answer):\n",
    "    # Encoder 입력\n",
    "    enc_i = sp.encode_as_ids(Q)\n",
    "    enc_input.append(enc_i)\n",
    "\n",
    "    # Decoder 입력, 출력\n",
    "    dec_i = [sp.bos_id()]   # <BOS>에서 시작함\n",
    "    dec_o = []\n",
    "    for ans in sp.encode_as_ids(A):\n",
    "        dec_i.append(ans)\n",
    "        dec_o.append(ans)\n",
    "    dec_o.append(sp.eos_id())   # Encoder 출력은 <EOS>로 끝남.        \n",
    "    \n",
    "    # dec_o는 <EOS>가 마지막에 들어있다. 나중에 pad_sequences()에서 <EOS>가\n",
    "    # 잘려 나가지 않도록 MAX_LEN 위치에 <EOS>를 넣어준다.\n",
    "    if len(dec_o) > MAX_LEN:\n",
    "        dec_o[MAX_LEN] = sp.eos_id()\n",
    "        \n",
    "    dec_input.append(dec_i)\n",
    "    dec_output.append(dec_o)\n",
    "\n",
    "# 각 문장의 길이를 맞추고 남는 부분에 padding을 삽입한다.\n",
    "enc_input = pad_sequences(enc_input, maxlen=MAX_LEN, value = sp.pad_id(), padding='post', truncating='post')\n",
    "dec_input = pad_sequences(dec_input, maxlen=MAX_LEN, value = sp.pad_id(), padding='post', truncating='post')\n",
    "dec_output = pad_sequences(dec_output, maxlen=MAX_LEN, value = sp.pad_id(), padding='post', truncating='post')\n",
    "\n",
    "# 사전과 학습 데이터를 저장한다.\n",
    "with open('data/chatbot_voc.pkl', 'wb') as f:\n",
    "    pickle.dump([word2idx, idx2word], f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# BLEU 평가를 위해 que_test와 ans_test를 저장해 둔다.\n",
    "with open('data/chatbot_train.pkl', 'wb') as f:\n",
    "    pickle.dump([enc_input, dec_input, dec_output], f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2318d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.layers import Embedding, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# 작업 디렉토리를 변경한다.\n",
    "# %cd '/content/drive/My Drive/Colab Notebooks'\n",
    "\n",
    "# Sub-word 사전 읽어온다.\n",
    "with open('data/chatbot_voc.pkl', 'rb') as f:\n",
    "    word2idx,  idx2word = pickle.load(f)\n",
    "\n",
    "# 학습 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 읽어온다.\n",
    "with open('data/chatbot_train.pkl', 'rb') as f:\n",
    "    trainXE, trainXD, trainYD = pickle.load(f)\n",
    "\t\n",
    "VOCAB_SIZE = len(idx2word)\n",
    "EMB_SIZE = 128\n",
    "LSTM_HIDDEN = 128\n",
    "MODEL_PATH = 'data/chatbot_trained.h5'\n",
    "LOAD_MODEL = False\n",
    "\n",
    "trainYD[0]\n",
    "\n",
    "print([idx2word[i] for i in trainYD[0]])\n",
    "\n",
    "# 워드 임베딩 레이어. Encoder와 decoder에서 공동으로 사용한다.\n",
    "K.clear_session()\n",
    "wordEmbedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMB_SIZE)\n",
    "\n",
    "# Encoder\n",
    "# -------\n",
    "# many-to-one으로 구성한다. 중간 출력은 필요 없고 decoder로 전달할 h와 c만\n",
    "# 필요하다. h와 c를 얻기 위해 return_state = True를 설정한다.\n",
    "encoderX = Input(batch_shape=(None, trainXE.shape[1]))\n",
    "encEMB = wordEmbedding(encoderX)\n",
    "encLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
    "encLSTM2 = LSTM(LSTM_HIDDEN, return_state = True)\n",
    "ey1, eh1, ec1 = encLSTM1(encEMB)    # LSTM 1층 \n",
    "_, eh2, ec2 = encLSTM2(ey1)       # LSTM 2층\n",
    "\n",
    "# Decoder\n",
    "# -------\n",
    "# many-to-many로 구성한다. target을 학습하기 위해서는 중간 출력이 필요하다.\n",
    "# 그리고 초기 h와 c는 encoder에서 출력한 값을 사용한다 (initial_state)\n",
    "# 최종 출력은 vocabulary의 인덱스인 one-hot 인코더이다.\n",
    "decoderX = Input(batch_shape=(None, trainXD.shape[1]))\n",
    "decEMB = wordEmbedding(decoderX)\n",
    "decLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
    "decLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
    "dy1, _, _ = decLSTM1(decEMB, initial_state = [eh1, ec1])\n",
    "dy2, _, _ = decLSTM2(dy1, initial_state = [eh2, ec2])\n",
    "decOutput = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))\n",
    "outputY = decOutput(dy2)\n",
    "\n",
    "# Model\n",
    "# -----\n",
    "model = Model([encoderX, decoderX], outputY)\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.0005), \n",
    "              loss='sparse_categorical_crossentropy')\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model.load_weights(MODEL_PATH)\n",
    "model.summary()\n",
    "\n",
    "# 학습 (teacher forcing)\n",
    "# ----------------------\n",
    "hist = model.fit([trainXE, trainXD], trainYD, batch_size = 512, epochs=300, shuffle=True)\n",
    "\n",
    "# 학습 결과를 저장한다\n",
    "model.save_weights(MODEL_PATH)\n",
    "\n",
    "# Loss history를 그린다\n",
    "plt.plot(hist.history['loss'], label='Train loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
