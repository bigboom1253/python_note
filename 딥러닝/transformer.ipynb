{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1203d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow.keras.backend as K\n",
    "import sentencepiece as spm\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from transformer import Encoder, Decoder, PaddingMask, PaddingAndLookaheadMask\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %cd '/content/drive/My Drive/Colab Notebooks'\n",
    "\n",
    "# Sub-word 사전을 읽어온다.\n",
    "with open('data/chatbot_voc.pkl', 'rb') as f:\n",
    "    word2idx,  idx2word = pickle.load(f)\n",
    "\n",
    "MAX_LEN = 15\n",
    "MODEL_PATH = 'data/transformer_model.h5'\n",
    "SPM_MODEL = \"data/chatbot_model.model\"\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(SPM_MODEL)\n",
    "\n",
    "# Model\n",
    "# -----\n",
    "K.clear_session()\n",
    "\n",
    "# 인코더 입력 (source). 입력 문장의 길이는 MAX_LEN으로 고정돼 있다.\n",
    "src = Input(batch_shape = (None, MAX_LEN), dtype=\"int32\", name=\"src\")\n",
    "\n",
    "# 디코더 입력 (target). 처음에는 [[<BOS>]], 그 다음은 [[<BOS>, 첫번째 예측값]], ...\n",
    "# 마지막 차원이 1에서 시작해서 하나씩 증가한다. 따라서 input shape에 마지막 차원을 지정하지 않는다.\n",
    "tar = Input(batch_shape = (None, None), dtype=\"int32\", name=\"tar\")\n",
    "\n",
    "# Encoder\n",
    "# -------\n",
    "padding_mask = PaddingMask()(src)\n",
    "encoder = Encoder(num_layers=4, d_model=128, num_heads=8, d_ff=512, vocab_size=VOCAB_SIZE, dropout_rate=0.1)\n",
    "enc_output, _ = encoder(src, padding_mask)\n",
    "\n",
    "# Decoder\n",
    "# -------\n",
    "lookahead_mask = PaddingAndLookaheadMask()(tar)\n",
    "decoder = Decoder(num_layers=4, d_model=128, num_heads=8, d_ff=512, vocab_size=VOCAB_SIZE, dropout_rate=0.1)\n",
    "dec_output, _, _ = decoder(tar, enc_output, lookahead_mask, padding_mask)\n",
    "\n",
    "# Final output\n",
    "final_output = Dense(VOCAB_SIZE, activation='softmax')(dec_output)\n",
    "\n",
    "model = Model(inputs=[src, tar], outputs=final_output)\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy')\n",
    "model.load_weights(MODEL_PATH)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Question을 입력받아 Answer를 생성한다.\n",
    "def genAnswer(question):\n",
    "    question = question[np.newaxis, :]\n",
    "    target = np.array(sp.bos_id()).reshape(1, 1)\n",
    "\n",
    "    answer = []\n",
    "    for i in range(MAX_LEN):\n",
    "        preds = model.predict_on_batch([question, target])\n",
    "        \n",
    "        # 디코더의 출력은 vocabulary에 대응되는 one-hot이다.\n",
    "        # argmax로 해당 단어를 채택한다.\n",
    "        nextWord = np.argmax(preds[:, -1:, :], axis=-1)\n",
    "        \n",
    "        # 예상 단어가 <EOS>이거나 <PAD>이면 더 이상 예상할 게 없다.\n",
    "        if nextWord == sp.eos_id() or nextWord == sp.pad_id():\n",
    "            break\n",
    "\n",
    "        # 다음 예상 단어인 디코더의 출력을 answer에 추가한다.\n",
    "        answer.append(idx2word[nextWord[0][0]])\n",
    "        \n",
    "        # 다음 target을 준비한다.\n",
    "        target = np.concatenate([target, nextWord], axis = -1)\n",
    "    \n",
    "    return sp.decode_pieces(answer)\n",
    "\n",
    "# Chatting\n",
    "def chatting(n=100):   \n",
    "    for i in range(n):\n",
    "        question = input('Q : ')\n",
    "        \n",
    "        if  question == 'quit':\n",
    "            break\n",
    "        \n",
    "        q_idx = []\n",
    "        for x in sp.encode_as_pieces(question):\n",
    "            if x in word2idx:\n",
    "                q_idx.append(word2idx[x])\n",
    "            else:\n",
    "                q_idx.append(sp.unk_id())   # out-of-vocabulary (OOV)\n",
    "        \n",
    "        # <PAD>를 삽입한다.\n",
    "        if len(q_idx) < MAX_LEN:\n",
    "            q_idx.extend([sp.pad_id()] * (MAX_LEN - len(q_idx)))\n",
    "        else:\n",
    "            q_idx = q_idx[0:MAX_LEN]\n",
    "\n",
    "        answer = genAnswer(np.array(q_idx))\n",
    "        print('A :', answer)\n",
    "\n",
    "####### Chatting 시작 #######\n",
    "print(\"\\nTransformer ChatBot (ver. 1.0)\")\n",
    "print(\"Chatting 모듈을 로드하고 있습니다 ...\")\n",
    "\n",
    "# 처음 1회는 시간이 걸리기 때문에 dummy question을 입력한다.\n",
    "# answer = genAnswer(np.zeros(MAX_LEN))\n",
    "print(\"ChatBot이 준비 됐습니다.\\n\")\n",
    "\n",
    "chatting(100)\n",
    "\n",
    "# train data:\n",
    "# [('이별 후 1년 그리고 선물', '이별하신게 맞나요'),\n",
    "#  ('허기져', '챙겨 드세요'),\n",
    "#  ('맥주 소주 어떤거 마실까', '소맥이요'),\n",
    "#  ('교양 수업 재밌어', '저도 듣고 싶어요'),\n",
    "#  ('권태기 이별', '극복하거나 이별하거나 둘 중 하나죠'),\n",
    "#  ('읽씹은 아프네', '상대방에 대한 예의가 없네요'),\n",
    "#  ('신혼여행 어디로 갈까', '못 가본 곳으로 가보세요'),\n",
    "#  ('반 배정 잘 될까', '잘 되길 바랍니다'),\n",
    "#  ('친구가 다 떠나서 내가 못났나 싶어', '지난 인연에 연연해하지 마세요'),\n",
    "#  ('뒤돌아 보지 말고 나가야 하는데', '살짝 뒤돌아봐도 괜찮아요')]\n",
    "\n",
    "# test data:\n",
    "# [('소오름 쫙', '좋은 일이길 바랍니다'),\n",
    "#  ('고백은 어떻게 하는거야', '솔직한 마음으로 다가가는 거죠'),\n",
    "#  ('참 잘낫네', '진정하셔요'),\n",
    "#  ('늘 빡빡하게 살기 힘드네', '여유가 생기길 바랍니다'),\n",
    "#  ('집까지 데려다줬는데 호감 그냥 매너', '호감이 있을 수도 있어요 그렇지만 조금 더 상황을 지켜보세요'),\n",
    "#  ('짝녀가 연락 안 되고 있는데 자나', '자고 있을지도 모르겠어요'),\n",
    "#  ('마음도 춥고 날씨도 춥고', '마음 감기 조심하세요'),\n",
    "#  ('죽었던 연애세포가 살아나는 것 같아', '좋은 소식이네요'),\n",
    "#  ('겨울에는 온천이지', '몸은 뜨겁고 머리는 차갑게'),\n",
    "#  ('소개팅 하고싶다', '친구한테 부탁해보세요')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02565a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.layers import Layer, LayerNormalization\n",
    "from tensorflow.keras.layers import Permute, Reshape\n",
    "\n",
    "class Transformer:\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 d_ff,\n",
    "                 input_vocab_size,\n",
    "                 target_vocab_size,\n",
    "                 dropout_rate,\n",
    "                 ffn_activation=tf.keras.activations.relu,\n",
    "                 scope=\"transformer\"):\n",
    "        self.encoder = Encoder(num_layers=num_layers,\n",
    "                               d_model=d_model,\n",
    "                               num_heads=num_heads,\n",
    "                               d_ff=d_ff,\n",
    "                               vocab_size=input_vocab_size,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               ffn_activation=ffn_activation,\n",
    "                               scope=\"%s/encoder\" % scope)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers,\n",
    "                               d_model=d_model,\n",
    "                               num_heads=num_heads,\n",
    "                               d_ff=d_ff,\n",
    "                               vocab_size=target_vocab_size,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               ffn_activation=ffn_activation,\n",
    "                               scope=\"%s/decoder\" % scope)\n",
    "\n",
    "        self.final_layer = Dense(target_vocab_size,\n",
    "                                 activation='softmax',\n",
    "                                 name=\"%s/dense\" % scope)\n",
    "\n",
    "        self.padding_mask = PaddingMask(name=\"%s/padding_mask\" % scope)\n",
    "        self.lookahead_mask = PaddingAndLookaheadMask(\n",
    "            name=\"%s/lookahead_mask\" % scope)\n",
    "\n",
    "    def __call__(self, inputs, target):\n",
    "        padding_mask = self.padding_mask(inputs)\n",
    "        lookahead_mask = self.lookahead_mask(target)\n",
    "\n",
    "        enc_output, enc_attention = self.encoder(inputs, padding_mask)\n",
    "\n",
    "        dec_output, dec_attention, enc_dec_attention = self.decoder(\n",
    "            target, enc_output, lookahead_mask, padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, enc_attention, dec_attention, enc_dec_attention\n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 d_ff,\n",
    "                 vocab_size,\n",
    "                 dropout_rate,\n",
    "                 ffn_activation=tf.keras.activations.relu,\n",
    "                 scope=\"decoder\"):\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.scope = scope\n",
    "\n",
    "        self.embedding = Embedding(input_dim=vocab_size,\n",
    "                                   output_dim=d_model,\n",
    "                                   name=\"%s/embedding\" % scope)\n",
    "        self.pos_encoding = PositionalEncoding(d_model,\n",
    "                                               name=\"%s/positional_encoding\" %\n",
    "                                               scope)\n",
    "\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         d_ff=d_ff,\n",
    "                         dropout_rate=dropout_rate,\n",
    "                         ffn_activation=ffn_activation,\n",
    "                         scope=\"%s/decoder_layer_%d\" % (scope, i))\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate, name=\"%s/dropout\" % self.scope)\n",
    "\n",
    "    def __call__(self, x, enc_output, lookahead_mask, padding_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = MultiplyConstant(self.d_model, name=\"%s/multiply\" % self.scope)(x)\n",
    "        x = Add(name=\"%s/add\" % self.scope)([x, self.pos_encoding(x)])\n",
    "\n",
    "        dec_attention_weights = {}\n",
    "        enc_dec_attention_weights = {}\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, dec_attention, enc_dec_attention = self.dec_layers[i](\n",
    "                x, enc_output, lookahead_mask, padding_mask)\n",
    "\n",
    "            dec_attention_weights[\"layer_%d\" % i] = dec_attention\n",
    "            enc_dec_attention_weights[\"layer_%d\" % i] = enc_dec_attention\n",
    "\n",
    "        return x, dec_attention_weights, enc_dec_attention_weights\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 d_ff,\n",
    "                 vocab_size,\n",
    "                 dropout_rate,\n",
    "                 ffn_activation=tf.keras.activations.relu,\n",
    "                 scope=\"encoder\"):\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.scope = scope\n",
    "\n",
    "        self.embedding = Embedding(input_dim=vocab_size,\n",
    "                                   output_dim=d_model,\n",
    "                                   name=\"%s/embedding\" % scope)\n",
    "        self.pos_encoding = PositionalEncoding(d_model,\n",
    "                                               name=\"%s/positional_encoding\" %\n",
    "                                               scope)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         d_ff=d_ff,\n",
    "                         dropout_rate=dropout_rate,\n",
    "                         ffn_activation=ffn_activation,\n",
    "                         scope=\"%s/encoder_layer_%d\" % (scope, i))\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate, name=\"%s/dropout\" % self.scope)\n",
    "\n",
    "    def __call__(self, x, padding_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = MultiplyConstant(self.d_model, name=\"%s/multiply\" % self.scope)(x)\n",
    "        x = Add(name=\"%s/add\" % self.scope)([x, self.pos_encoding(x)])\n",
    "\n",
    "        enc_attention_weights = {}\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, enc_attention = self.enc_layers[i](x, padding_mask)\n",
    "            enc_attention_weights[\"layer_%d\" % i] = enc_attention\n",
    "\n",
    "        return x, enc_attention_weights\n",
    "\n",
    "\n",
    "class DecoderLayer:\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 d_ff,\n",
    "                 dropout_rate,\n",
    "                 ffn_activation=tf.keras.activations.relu,\n",
    "                 scope=\"decoder_layer\"):\n",
    "        self.scope = scope\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model,\n",
    "                                       num_heads,\n",
    "                                       scope=\"%s/multi_head_attention_1\" %\n",
    "                                       scope)\n",
    "        self.mha2 = MultiHeadAttention(d_model,\n",
    "                                       num_heads,\n",
    "                                       scope=\"%s/multi_head_attention_2\" %\n",
    "                                       scope)\n",
    "        self.ffn = PointwiseFeedForwardNetwork(\n",
    "            d_model,\n",
    "            d_ff,\n",
    "            activation=ffn_activation,\n",
    "            scope=\"%s/pointwise_feed_forward_network\" % scope)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6,\n",
    "                                             name=\"%s/layer_norm_1\" % scope)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6,\n",
    "                                             name=\"%s/layer_norm_2\" % scope)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6,\n",
    "                                             name=\"%s/layer_norm_3\" % scope)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate, name=\"%s/dropout_1\" % scope)\n",
    "        self.dropout2 = Dropout(dropout_rate, name=\"%s/dropout_2\" % scope)\n",
    "        self.dropout3 = Dropout(dropout_rate, name=\"%s/dropout_3\" % scope)\n",
    "\n",
    "    def __call__(self, x, enc_output, lookahead_mask, padding_mask):\n",
    "        out1, dec_dec_attention = self.mha1(x, x, x, lookahead_mask)\n",
    "        out1 = self.dropout1(out1)\n",
    "        x = Add(name=\"%s/add_1\" % self.scope)([x, out1])\n",
    "        x = self.layernorm1(x)\n",
    "\n",
    "        out2, enc_dec_attention = self.mha2(x, enc_output, enc_output,\n",
    "                                            padding_mask)\n",
    "        out2 = self.dropout2(out2)\n",
    "        x = Add(name=\"%s/add_2\" % self.scope)([x, out2])\n",
    "        x = self.layernorm2(x)\n",
    "\n",
    "        ffn_output = self.ffn(x)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        x = Add(name=\"%s/add_3\" % self.scope)([x, ffn_output])\n",
    "        x = self.layernorm3(x)\n",
    "\n",
    "        return x, dec_dec_attention, enc_dec_attention\n",
    "\n",
    "class EncoderLayer:\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 d_ff,\n",
    "                 dropout_rate,\n",
    "                 ffn_activation=tf.keras.activations.relu,\n",
    "                 scope=\"encoder_layer\"):\n",
    "        self.scope = scope\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model,\n",
    "                                       num_heads,\n",
    "                                       scope=\"%s/multi_head_attention_1\" %\n",
    "                                       scope)\n",
    "        self.ffn = PointwiseFeedForwardNetwork(\n",
    "            d_model,\n",
    "            d_ff,\n",
    "            activation=ffn_activation,\n",
    "            scope=\"%s/pointwise_feed_forward_network\" % scope)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6,\n",
    "                                             name=\"%s/layer_norm_1\" % scope)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6,\n",
    "                                             name=\"%s/layer_norm_2\" % scope)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate, name=\"%s/dropout_1\" % scope)\n",
    "        self.dropout2 = Dropout(dropout_rate, name=\"%s/dropout_2\" % scope)\n",
    "\n",
    "    def __call__(self, x, padding_mask):\n",
    "        out1, enc_enc_attention = self.mha1(x, x, x, padding_mask)\n",
    "        out1 = self.dropout1(out1)\n",
    "        x = Add(name=\"%s/add_1\" % self.scope)([x, out1])\n",
    "        x = self.layernorm1(x)\n",
    "\n",
    "        ffn_output = self.ffn(x)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        x = Add(name=\"%s/add_2\" % self.scope)([x, ffn_output])\n",
    "        x = self.layernorm2(x)\n",
    "\n",
    "        return x, enc_enc_attention\n",
    "\n",
    "\n",
    "class PointwiseFeedForwardNetwork:\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 d_ff,\n",
    "                 activation=tf.keras.activations.relu,\n",
    "                 scope=\"pointwise_feed_forward_network\"):\n",
    "        self.dense_1 = Dense(d_ff,\n",
    "                             activation=activation,\n",
    "                             name=\"%s/dense_1\" % scope)\n",
    "        self.dense_2 = Dense(d_model,\n",
    "                             activation=None,\n",
    "                             name=\"%s/dense_2\" % scope)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.dense_2(self.dense_1(x))\n",
    "\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads, scope=\"multi_head_attention\"):\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.wq = Dense(d_model, name=\"%s/dense_q\" % scope)\n",
    "        self.wk = Dense(d_model, name=\"%s/dense_k\" % scope)\n",
    "        self.wv = Dense(d_model, name=\"%s/dense_v\" % scope)\n",
    "\n",
    "        self.reshapeq = Reshape((-1, num_heads, d_model // num_heads),\n",
    "                                name=\"%s/reshape_q\" % scope)\n",
    "        self.reshapek = Reshape((-1, num_heads, d_model // num_heads),\n",
    "                                name=\"%s/reshape_k\" % scope)\n",
    "        self.reshapev = Reshape((-1, num_heads, d_model // num_heads),\n",
    "                                name=\"%s/reshape_v\" % scope)\n",
    "\n",
    "        self.transposeq = Permute((2, 1, 3), name=\"%s/transpose_q\" % scope)\n",
    "        self.transposek = Permute((2, 1, 3), name=\"%s/transpose_k\" % scope)\n",
    "        self.transposev = Permute((2, 1, 3), name=\"%s/transpose_v\" % scope)\n",
    "\n",
    "        self.reshape_output = Reshape((-1, d_model),\n",
    "                                      name=\"%s/reshape_output\" % scope)\n",
    "\n",
    "        self.transpose_output = Permute((2, 1, 3),\n",
    "                                        name=\"%s/transpose_output\" % scope)\n",
    "\n",
    "        self.dense = Dense(d_model, name=\"%s/dense\" % scope)\n",
    "\n",
    "        self.attention = Attention(name=\"%s/attention\" % scope)\n",
    "\n",
    "    def __call__(self, q, k, v, mask):\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.reshapeq(q)\n",
    "        k = self.reshapek(k)\n",
    "        v = self.reshapev(v)\n",
    "\n",
    "        q = self.transposeq(q)\n",
    "        k = self.transposek(k)\n",
    "        v = self.transposev(v)\n",
    "\n",
    "        x, attention_weights = self.attention([q, k, v, mask])\n",
    "\n",
    "        x = self.transpose_output(x)\n",
    "        x = self.reshape_output(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return x, attention_weights\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __call__(self, inputs):\n",
    "        q, k, v, mask = inputs\n",
    "\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        scaled_attention_logits += mask * -1e9\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, d_model, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        position = tf.shape(inputs)[1]\n",
    "\n",
    "        position_dims = tf.range(position)[:, tf.newaxis]\n",
    "        embed_dims = tf.range(self.d_model)[tf.newaxis, :]\n",
    "        angle_rates = 1 / tf.pow(\n",
    "            10000.0, tf.cast(\n",
    "                (2 * (embed_dims // 2)) / self.d_model, tf.float32))\n",
    "        angle_rads = tf.cast(position_dims, tf.float32) * angle_rates\n",
    "\n",
    "        sines = tf.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        return dict(list(base.items()) + [(\"d_model\", self.d_model)])\n",
    "\n",
    "\n",
    "class MultiplyConstant(Layer):\n",
    "    def __init__(self, c, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.c = c\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return inputs * self.c\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        return dict(list(base.items()) + [(\"c\", self.c)])\n",
    "\n",
    "\n",
    "class PaddingMask(Layer):\n",
    "    def __call__(self, inputs):\n",
    "        seq = tf.cast(tf.math.equal(inputs, 0), tf.float32)\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "class PaddingAndLookaheadMask(Layer):\n",
    "    def __call__(self, inputs):\n",
    "        size = tf.shape(inputs)[1]\n",
    "        lhm = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "\n",
    "        seq = tf.cast(tf.math.equal(inputs, 0), tf.float32)\n",
    "        seq = seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "        return tf.maximum(lhm, seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
