{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af6b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq-Attention Machine Translator : 학습 데이터 모듈\n",
    "# Google의 Sentencepiece를 이용해서 학습 데이터를 생성한다.\n",
    "#\n",
    "# 저작자: 2021.08.02, 조성현 (blog.naver.com/chunjein)\n",
    "# copyright: SNS 등에 공개할 때는 출처에 저작자를 명시해 주시기 바랍니다.\n",
    "# -------------------------------------------------------------------\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# 작업 디렉토리를 변경한다.\n",
    "# %cd '/content/drive/My Drive/Colab Notebooks'\n",
    "\n",
    "# 데이터 파일을 읽어온다.\n",
    "df = pd.read_csv('data/machine_trans.csv', header=0)\n",
    "source, target = list(df['source']), list(df['target'])\n",
    "df.head()\n",
    "\n",
    "# 학습 데이터와 시험 데이터를 분리한다.\n",
    "src_train, src_test, tar_train, tar_test = train_test_split(source, target, test_size=0.1, random_state=0)\n",
    "\n",
    "src_train[0], tar_train[0]\n",
    "\n",
    "# Google의 Sentencepiece를 이용해서 vocabulary를 생성한다.\n",
    "# -----------------------------------------------------\n",
    "templates= \"--input={} \\\n",
    "            --pad_id=0 --pad_piece=<PAD>\\\n",
    "            --unk_id=1 --unk_piece=<UNK>\\\n",
    "            --bos_id=2 --bos_piece=<BOS>\\\n",
    "            --eos_id=3 --eos_piece=<EOS>\\\n",
    "            --model_prefix={} \\\n",
    "            --vocab_size={}\"\n",
    "\n",
    "# Sentencepice용 한글 (source) 사전을 만들기 위해 src_train + src_test를 저장해 둔다.\n",
    "data_file = \"data/mt_source.txt\"\n",
    "with open(data_file, 'w', encoding='utf-8') as f:\n",
    "    for sent in src_train + src_test:\n",
    "        f.write(sent + '\\n')\n",
    "\n",
    "SRC_VOCAB = 9000\n",
    "model_prefix = \"data/source_model\"\n",
    "params = templates.format(data_file, model_prefix, SRC_VOCAB)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(params)\n",
    "sp_source = spm.SentencePieceProcessor()\n",
    "sp_source.Load(model_prefix + '.model')\n",
    "\n",
    "with open(model_prefix + '.vocab', encoding='utf-8') as f:\n",
    "    vocab = [doc.strip().split('\\t') for doc in f]\n",
    "\n",
    "src_word2idx = {k:v for v, [k, _] in enumerate(vocab)}\n",
    "src_idx2word = {v:k for v, [k, _] in enumerate(vocab)}\n",
    "\n",
    "print([src_idx2word[i] for i in range(20)])\n",
    "\n",
    "# Sentencepice용 영어 (target) 사전을 만들기 위해 tar_train + tar_test를 저장해 둔다.\n",
    "data_file = \"data/mt_target.txt\"\n",
    "with open(data_file, 'w', encoding='utf-8') as f:\n",
    "    for sent in tar_train + tar_test:\n",
    "        f.write(sent + '\\n')\n",
    "\n",
    "TAR_VOCAB = 4798\n",
    "model_prefix = \"data/target_model\"\n",
    "params = templates.format(data_file, model_prefix, TAR_VOCAB)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(params)\n",
    "sp_target = spm.SentencePieceProcessor()\n",
    "sp_target.Load(model_prefix + '.model')\n",
    "\n",
    "with open(model_prefix + '.vocab', encoding='utf-8') as f:\n",
    "    vocab = [doc.strip().split('\\t') for doc in f]\n",
    "\n",
    "tar_word2idx = {k:v for v, [k, _] in enumerate(vocab)}\n",
    "tar_idx2word = {v:k for v, [k, _] in enumerate(vocab)}\n",
    "\n",
    "print([tar_idx2word[i] for i in range(20)])\n",
    "\n",
    "# 학습 데이터를 생성한다. (인코더 입력용, 디코더 입력용, 디코더 출력용)\n",
    "MAX_LEN_SRC = 15\n",
    "MAX_LEN_TAR = 20\n",
    "enc_input = []\n",
    "dec_input = []\n",
    "dec_output = []\n",
    "\n",
    "for src, tar in zip(src_train, tar_train):\n",
    "    # Encoder 입력\n",
    "    enc_i = sp_source.encode_as_ids(src)\n",
    "    enc_input.append(enc_i)\n",
    "\n",
    "    # Decoder 입력, 출력\n",
    "    dec_i = [sp_target.bos_id()]   # <BOS>에서 시작함\n",
    "    dec_o = []\n",
    "    for ans in sp_target.encode_as_ids(tar):\n",
    "        dec_i.append(ans)\n",
    "        dec_o.append(ans)\n",
    "    dec_o.append(sp_target.eos_id())   # Encoder 출력은 <EOS>로 끝남.        \n",
    "    \n",
    "    # dec_o는 <EOS>가 마지막에 들어있다. 나중에 pad_sequences()에서 <EOS>가\n",
    "    # 잘려 나가지 않도록 MAX_LEN 위치에 <EOS>를 넣어준다.\n",
    "    if len(dec_o) > MAX_LEN_TAR:\n",
    "        dec_o[MAX_LEN_TAR] = sp_target.eos_id()\n",
    "        \n",
    "    dec_input.append(dec_i)\n",
    "    dec_output.append(dec_o)\n",
    "\n",
    "# 문장 길이 분포를 파악한다.\n",
    "sns.displot([len(s) for s in dec_input])\n",
    "\n",
    "# 각 문장의 길이를 맞추고 남는 부분에 padding을 삽입한다.\n",
    "enc_input = pad_sequences(enc_input, maxlen=MAX_LEN_SRC, value = sp_source.pad_id(), padding='post', truncating='post')\n",
    "dec_input = pad_sequences(dec_input, maxlen=MAX_LEN_TAR, value = sp_target.pad_id(), padding='post', truncating='post')\n",
    "dec_output = pad_sequences(dec_output, maxlen=MAX_LEN_TAR, value = sp_target.pad_id(), padding='post', truncating='post')\n",
    "\n",
    "# 사전과 학습 데이터를 저장한다.\n",
    "with open('data/mt_voc.pkl', 'wb') as f:\n",
    "    pickle.dump([src_word2idx, src_idx2word, tar_word2idx, tar_idx2word], f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# BLEU 평가를 위해 que_test와 ans_test를 저장해 둔다.\n",
    "with open('data/mt_train.pkl', 'wb') as f:\n",
    "    pickle.dump([enc_input, dec_input, dec_output, src_test, tar_test], f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "enc_input[0]\n",
    "\n",
    "dec_input[0]\n",
    "\n",
    "dec_output[0]\n",
    "\n",
    "[(s, t) for s, t in zip(src_train[:10], tar_train[:10])]\n",
    "\n",
    "[(s, t) for s, t in zip(src_test[:10], tar_test[:10])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac23980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dot, Concatenate\n",
    "from tensorflow.keras.layers import Embedding, TimeDistributed, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# 작업 디렉토리를 변경한다.\n",
    "# %cd '/content/drive/My Drive/Colab Notebooks'\n",
    "\n",
    "# 서브워드 사전을 읽어온다.\n",
    "with open('data/mt_voc.pkl', 'rb') as f:\n",
    "    src_word2idx,  src_idx2word, tar_word2idx, tar_idx2word = pickle.load(f)\n",
    "    \n",
    "# 학습 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 읽어온다.\n",
    "with open('data/mt_train.pkl', 'rb') as f:\n",
    "    trainXE, trainXD, trainYD, _, _ = pickle.load(f)\n",
    "\n",
    "SRC_VOCAB = len(src_idx2word)\n",
    "TAR_VOCAB = len(tar_idx2word)\n",
    "EMB_SIZE = 128\n",
    "LSTM_HIDDEN = 128\n",
    "MODEL_PATH = 'data/mt_attention.h5'\n",
    "LOAD_MODEL = True\n",
    "\n",
    "# Seq2Seq-Attention 모델을 생성한다.\n",
    "K.clear_session()\n",
    "\n",
    "# Encoder\n",
    "# -------\n",
    "# many-to-many로 구성한다. Attention value를 계산하기 위해 중간 출력이 필요하고\n",
    "# (return_sequences=True), decoder로 전달할 h와 c도 필요하다 (return_state = True)\n",
    "encoderX = Input(batch_shape=(None, trainXE.shape[1]))\n",
    "encEMB = Embedding(input_dim=SRC_VOCAB, output_dim=EMB_SIZE)(encoderX)\n",
    "encLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
    "encLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
    "ey1, eh1, ec1 = encLSTM1(encEMB)    # LSTM 1층 \n",
    "ey2, eh2, ec2 = encLSTM2(ey1)       # LSTM 2층\n",
    "\n",
    "# Decoder\n",
    "# -------\n",
    "# many-to-many로 구성한다. target을 학습하기 위해서는 중간 출력이 필요하다.\n",
    "# 그리고 초기 h와 c는 encoder에서 출력한 값을 사용한다 (initial_state)\n",
    "# 최종 출력은 vocabulary의 인덱스인 one-hot 인코더이다.\n",
    "decoderX = Input(batch_shape=(None, trainXD.shape[1]))\n",
    "decEMB = Embedding(input_dim=TAR_VOCAB, output_dim=EMB_SIZE)(decoderX)\n",
    "decLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
    "decLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
    "dy1, _, _ = decLSTM1(decEMB, initial_state = [eh1, ec1])\n",
    "dy2, _, _ = decLSTM2(dy1, initial_state = [eh2, ec2])\n",
    "decOutput = TimeDistributed(Dense(TAR_VOCAB, activation='softmax'))\n",
    "outputY = decOutput(dy2)\n",
    "\n",
    "# Model\n",
    "# -----\n",
    "model = Model([encoderX, decoderX], outputY)\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model.load_weights(MODEL_PATH)\n",
    "model.summary()\n",
    "\n",
    "# 학습 (teacher forcing)\n",
    "hist = model.fit([trainXE, trainXD], trainYD, batch_size = 512, epochs=100, shuffle=True)\n",
    "\n",
    "# 학습 결과를 저장한다\n",
    "model.save_weights(MODEL_PATH)\n",
    "\n",
    "# Loss history를 그린다\n",
    "plt.plot(hist.history['loss'], label='Train loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6f8deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dot, Concatenate\n",
    "from tensorflow.keras.layers import Embedding, TimeDistributed, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 서브워드 사전을 읽어온다.\n",
    "with open('data/mt_voc.pkl', 'rb') as f:\n",
    "    src_word2idx,  src_idx2word, tar_word2idx, tar_idx2word = pickle.load(f)\n",
    "\n",
    "# 시험 데이터를 읽어온다.\n",
    "with open('data/mt_train.pkl', 'rb') as f:\n",
    "    _, _, _, src_test, tar_test = pickle.load(f)\n",
    "\n",
    "SRC_VOCAB = len(src_idx2word)\n",
    "TAR_VOCAB = len(tar_idx2word)\n",
    "EMB_SIZE = 128\n",
    "LSTM_HIDDEN = 128\n",
    "MAX_LEN_SRC = 15\n",
    "MAX_LEN_TAR = 20\n",
    "MODEL_PATH = 'data/mt_attention.h5'\n",
    "\n",
    "# 데이터 전처리 과정에서 생성한 SentencePiece model을 불러온다.\n",
    "sp_source = spm.SentencePieceProcessor()\n",
    "sp_target = spm.SentencePieceProcessor()\n",
    "sp_source.Load(\"data/source_model.model\")\n",
    "sp_target.Load(\"data/target_model.model\")\n",
    "\n",
    "# Seq2Seq-Attention 모델을 생성한다.\n",
    "K.clear_session()\n",
    "\n",
    "# Encoder\n",
    "# -------\n",
    "encoderX = Input(batch_shape=(None, MAX_LEN_SRC))\n",
    "encEMB = Embedding(input_dim=SRC_VOCAB, output_dim=EMB_SIZE)(encoderX)\n",
    "encLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
    "encLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state = True)\n",
    "ey1, eh1, ec1 = encLSTM1(encEMB)      # LSTM 1층 \n",
    "ey2, eh2, ec2 = encLSTM2(ey1)         # LSTM 2층\n",
    "\n",
    "# Decoder\n",
    "# -------\n",
    "# Decoder는 1개 단어씩을 입력으로 받는다. 학습 때와 달리 문장 전체를 받아\n",
    "# recurrent하는 것이 아니라, 단어 1개씩 입력 받아서 다음 예상 단어를 확인한다.\n",
    "# chatting()에서 for 문으로 단어 별로 recurrent 시킨다.\n",
    "# 따라서 batch_shape = (None, 1)이다. 즉, time_step = 1이다. 그래도 네트워크\n",
    "# 파라메터는 동일하다.\n",
    "decoderX = Input(batch_shape=(None, 1))\n",
    "decEMB = Embedding(input_dim=TAR_VOCAB, output_dim=EMB_SIZE)(decoderX)\n",
    "decLSTM1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
    "decLSTM2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
    "dy1, _, _ = decLSTM1(decEMB, initial_state = [eh1, ec1])\n",
    "dy2, _, _ = decLSTM2(dy1, initial_state = [eh2, ec2])\n",
    "decOutput = TimeDistributed(Dense(TAR_VOCAB, activation='softmax'))\n",
    "outputY = decOutput(dy2)\n",
    "\n",
    "# Model\n",
    "# -----\n",
    "model = Model([encoderX, decoderX], outputY)\n",
    "model.load_weights(MODEL_PATH)\n",
    "\n",
    "# 기계번역 model\n",
    "model_enc = Model(encoderX, [eh1, ec1, eh2, ec2, ey2])\n",
    "\n",
    "ih1 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
    "ic1 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
    "ih2 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
    "ic2 = Input(batch_shape = (None, LSTM_HIDDEN))\n",
    "ey = Input(batch_shape = (None, MAX_LEN_SRC, LSTM_HIDDEN))\n",
    "\n",
    "dec_output1, dh1, dc1 = decLSTM1(decEMB, initial_state = [ih1, ic1])\n",
    "dec_output2, dh2, dc2 = decLSTM2(dec_output1, initial_state = [ih2, ic2])\n",
    "dec_output = decOutput(dec_output2)\n",
    "model_dec = Model([decoderX, ih1, ic1, ih2, ic2, ey], \n",
    "                  [dec_output, dh1, dc1, dh2, dc2])\n",
    "\n",
    "# Source 문장을 입력받아 target문장을 생성한다.\n",
    "def genAnswer(source):\n",
    "    source = source[np.newaxis, :]\n",
    "    init_h1, init_c1, init_h2, init_c2, enc_y = model_enc.predict(source)\n",
    "\n",
    "    # 시작 단어는 <BOS>로 한다.\n",
    "    word = np.array(sp_target.bos_id()).reshape(1, 1)\n",
    "\n",
    "    target = []\n",
    "    for i in range(MAX_LEN_TAR):\n",
    "        dY, next_h1, next_c1, next_h2, next_c2 = \\\n",
    "            model_dec.predict([word, init_h1, init_c1, init_h2, init_c2, enc_y])\n",
    "        \n",
    "        # 디코더의 출력은 vocabulary에 대응되는 one-hot이다.\n",
    "        # argmax로 해당 단어를 채택한다.\n",
    "        nextWord = np.argmax(dY[0, 0])\n",
    "\n",
    "        # 예상 단어가 <EOS>이거나 <PAD>이면 더 이상 예상할 게 없다.\n",
    "        if nextWord == sp_target.eos_id() or nextWord == sp_target.pad_id():\n",
    "            break\n",
    "        \n",
    "        # 다음 예상 단어인 디코더의 출력을 target에 추가한다.\n",
    "        target.append(tar_idx2word[nextWord])\n",
    "        \n",
    "        # 디코더의 다음 recurrent를 위해 입력 데이터와 hidden 값을\n",
    "        # 준비한다. 입력은 word이고, hidden은 h와 c이다.\n",
    "        word = np.array(nextWord).reshape(1,1)\n",
    "    \n",
    "        init_h1 = next_h1\n",
    "        init_c1 = next_c1\n",
    "        init_h2 = next_h2\n",
    "        init_c2 = next_c2\n",
    "        \n",
    "    return sp_target.decode_pieces(target)\n",
    "\n",
    "def make_question(src_string):\n",
    "    src_idx = []\n",
    "    for x in sp_source.encode_as_pieces(src_string):\n",
    "        if x in src_word2idx:\n",
    "            src_idx.append(src_word2idx[x])\n",
    "        else:\n",
    "            src_idx.append(sp_source.unk_id())   # out-of-vocabulary (OOV)\n",
    "    \n",
    "    # <PAD>를 삽입한다.\n",
    "    if len(src_idx) < MAX_LEN_SRC:\n",
    "        src_idx.extend([sp_source.pad_id()] * (MAX_LEN_SRC - len(src_idx)))\n",
    "    else:\n",
    "        src_idx = src_idx[0:MAX_LEN_SRC]\n",
    "    return src_idx\n",
    "\n",
    "# 기계번역 수행.\n",
    "# dummy : 최초 1회는 모델을 로드하는데 약간의 시간이 걸리므로 이것을 가리기 위함.\n",
    "def translate(n=100):\n",
    "    for i in range(n):\n",
    "        source = input('Source : ')\n",
    "        \n",
    "        if  source == 'quit':\n",
    "            break\n",
    "        \n",
    "        src_idx = make_question(source)\n",
    "        target = genAnswer(np.array(src_idx))\n",
    "        print('Target :', target)\n",
    "\n",
    "####### 기계번역 시작 #######\n",
    "print(\"\\nSeq2Seq-Attention Machine Translator (ver. 1.0)\")\n",
    "print(\"기계번역 모듈을 로드하고 있습니다 ...\")\n",
    "\n",
    "# 처음 1회는 시간이 걸리기 때문에 dummy source를 입력한다.\n",
    "target = genAnswer(np.zeros(MAX_LEN_SRC))\n",
    "print(\"기계번역 모듈이 준비 됐습니다.\\n\")\n",
    "\n",
    "# 번역을 시작한다.\n",
    "translate(100)\n",
    "\n",
    "# train data:\n",
    "# [('요리하는게 재밌어', \"It's fun to cook\"),\n",
    "#  ('첫만남 그리고 끝', 'First meeting and end'),\n",
    "#  ('볼 수가 없군요', 'I can not see it.'),\n",
    "#  ('이상형은 아니지만 호감가는 사람이 있어요', \"I'm not an ideal, but I have a crush.\"),\n",
    "#  ('이별 준비 중', 'Preparing for separation'),\n",
    "#  ('자꾸 생각하면 더 힘들어요', \"It's harder to keep thinking.\"),\n",
    "#  ('내 집 마련 축하드려요', 'I am celebrating my house.'),\n",
    "#  ('헤어진지 43일', '43 days'),\n",
    "#  ('마음이 시키는데로 하면 되요', 'I want you to make it.'),\n",
    "#  ('사정이 뭔지 모르겠지만 좋아하는 건 괜찮아요',\n",
    "#   \"I do not know what the situation is, but I like it's okay.\")]\n",
    "\n",
    "# test data:\n",
    "# [('눈을 크게 뜨고 잘 찾아보세요', 'Find your eyes and find it well.'),\n",
    "#  ('나쁜 사람일지도 모르겠네요', 'I do not know a bad person.'),\n",
    "#  ('거창하지 않아도 돼요', 'I do not have to be a tremend.'),\n",
    "#  ('썸 타다 지쳤어', \"I'm tired of it.\"),\n",
    "#  ('사랑의 힘인가봐요', \"I think it's the power of love.\"),\n",
    "#  ('변하는 건 사랑이겠죠', \"It's love to change.\"),\n",
    "#  ('이제 짝사랑 말고 둘이서 하는 사랑하세요', 'I love you two,'),\n",
    "#  ('짝남한테 계속 친한 척하면 부담스러워 할까',\n",
    "#   'If you keep pretending to be close to the mild man, will it be burdensome?'),\n",
    "#  ('어른이 된다는 것', 'Manger'),\n",
    "#  ('꼭 사랑한다고 말해야 사랑하는건 아니라고 생각해요', 'I think I do not love you to say that I love you.')]\n",
    "\n",
    "src_test[0], tar_test[0]\n",
    "\n",
    "n = 50\n",
    "y_test = tar_test[:n]\n",
    "\n",
    "y_pred = []\n",
    "for src in tqdm(y_test, total=len(y_test)):\n",
    "    src_idx = make_question(src)\n",
    "    src_ans = genAnswer(np.array(src_idx))\n",
    "    y_pred.append(src_ans)\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "y_test1 = [[x.split()] for x in y_test]\n",
    "y_pred1 = [x.split() for x in y_pred]\n",
    "\n",
    "bleu_score = corpus_bleu(y_test1, y_pred1, weights=(1,0,0,0), smoothing_function=SmoothingFunction().method1)\n",
    "print('BLEU score =', bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b26ea9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372347f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
