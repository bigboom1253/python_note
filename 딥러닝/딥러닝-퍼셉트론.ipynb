{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ac0bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "981dfa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "\n",
    "x_feat = cancer['data']             ## 입력층에 입력, 구조 = 2D\n",
    "y_target = cancer['target'].reshape(-1, 1)         ## 출력층으로 출력될 data, 구조 = 2D  ,그냥cancer['target']은 1D구조라서 reshape(-1, 1)로 2D를 만들어준다.\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_feat, y_target, test_size= 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18939c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network을 생성한다.(위의 그림을 코드로 그리자!)\n",
    "\n",
    "# 입력층구성\n",
    "xInput =Input(batch_shape=(None, x_train.shape[1]))    # 입력할 data의 구조, None은 all의 의미, x_train의 데이터가 들어올꺼니깐 입력층을 이렇게 구성해라!\n",
    "\n",
    "# 은닉층 구성\n",
    "\n",
    "hLayer = Dense(10)(xInput)\n",
    "#은닉층 갯수 늘리는 뱡법\n",
    "# hLayer = Dense(10)(xInput) # 중간에 은닉층 10개 생성 # 입력층과 연결(xInput의 출력이 Dense의 입력으로 들어가서 은닉층의 출력인 hLayer가 출력된다.)\n",
    "# hLayer1 = Dense(10)(hLayer) \n",
    "# hLayer2 = Dense(10)(hLayer1) \n",
    "# hLayer3 = Dense(10)(hLayer2) \n",
    "# hLayer4 = Dense(10)(hLayer3)\n",
    "\n",
    "# 출력층 구성\n",
    "yOutput = Dense(y_train.shape[1], activation = 'sigmoid')(hLayer4) # 출력층은 출력뉴런이 1개 # binary classification 할때는 sigmoid를 쓴다->최종출력이 0~1이 나온다.\n",
    "                                                                  # 출력층을 은닉층과 연결\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "facdc1bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cancer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19132/1249047966.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcancer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cancer' is not defined"
     ]
    }
   ],
   "source": [
    "cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9e4b75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 0s 572us/step - loss: 280.4473\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 572us/step - loss: 192.2701\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 0s 500us/step - loss: 105.5305\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 572us/step - loss: 24.3600\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 715us/step - loss: 10.1982\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 714us/step - loss: 5.7518\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 572us/step - loss: 4.2908\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 500us/step - loss: 3.8648\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 572us/step - loss: 3.6324\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 643us/step - loss: 3.3634\n",
      "0.7543859649122807\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "x_feat = cancer['data']             ## 입력층에 입력, 구조 = 2D\n",
    "y_target = cancer['target'].reshape(-1, 1)         ## 출력층으로 출력될 data, 구조 = 2D  ,그냥cancer['target']은 1D구조라서 reshape(-1, 1)로 2D를 만들어준다.\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_feat, y_target, test_size= 0.2)\n",
    "\n",
    "# Network을 생성한다.(위의 그림을 코드로 그리자!)\n",
    "\n",
    "# 입력층구성\n",
    "xInput =Input(batch_shape=(None, x_train.shape[1]))    # 입력할 data의 구조, None은 all의 의미, x_train의 데이터가 들어올꺼니깐 입력층을 이렇게 구성해라!\n",
    "\n",
    "# 은닉층 구성\n",
    "\n",
    "hLayer = Dense(10)(xInput)\n",
    "#은닉층 갯수 늘리는 방법\n",
    "# hLayer = Dense(10)(xInput) # 중간에 은닉층 10개 생성 # 입력층과 연결(xInput의 출력이 Dense의 입력으로 들어가서 은닉층의 출력인 hLayer가 출력된다.)\n",
    "# hLayer1 = Dense(10)(hLayer) \n",
    "# hLayer2 = Dense(10)(hLayer1) \n",
    "# hLayer3 = Dense(10)(hLayer2) \n",
    "# hLayer4 = Dense(10)(hLayer3)\n",
    "\n",
    "# 출력층 구성\n",
    "yOutput = Dense(y_train.shape[1], activation = 'sigmoid')(hLayer) # 출력층은 출력뉴런이 1개 # binary classification 할때는 sigmoid를 쓴다->최종출력이 0~1이 나온다.\n",
    "                                                                  # 출력층을 은닉층과 연결\n",
    "#은닉층을 늘렸을 때 적용\n",
    "# yOutput = Dense(y_train.shape[1], activation = 'sigmoid')(hLayer4) # 출력층은 출력뉴런이 1개 # binary classification 할때는 sigmoid를 쓴다->최종출력이 0~1이 나온다.\n",
    "#                                                                   # 출력층을 은닉층과 연결\n",
    "\n",
    "# 모델을 만든다!\n",
    "\n",
    "model = Model(xInput, yOutput) # 입력층의 입력을 받아서 출력층의 출력이 나오는 모델! xInput을 입력 받아 yOutput을 출력한다.\n",
    "\n",
    "# 학습 방법을 설정한다.\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'adam') # 학습할 때 loss function을 binary_crossentropy를 써라! (binary classification인 경우는 loss function 을 binary_crossentropy)\n",
    "                                                              # optimizer = 'adam' : loss의 최소점을 찾아가는 방법을 지정\n",
    "\n",
    "# 학습\n",
    "\n",
    "# model.fit(x_train, y_train, epochs =100) # epochs는 반복횟수, 연결 가중치인 weights를 적절한 값을 updata한다.\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "# 평가\n",
    "\n",
    "y_prob =model.predict(x_test) # sigmoid를 썼기 때문에 출력이 0~1사이 값이 나온다,ex) 0.3, 0.8, 0.7...\n",
    "\n",
    "y_pred =(y_prob > 0.5).astype('int8')  # (y_prob > 0.5)의 출력은 F/T로 나온다. astype('int8')은 불리언형태를 숫자로 바꿔준다 ex) 0, 1, 1, ...\n",
    "\n",
    "acc= (y_test == y_pred).sum()/y_train.shape[0]\n",
    "acc= (y_test == y_pred).mean()\n",
    "print(acc)\n",
    "\n",
    "# 평가 결과가 만족스럽다면 활용. x_new 라는 data가 있다면 x_new에 해당하는 target을 추정할 수 있다.\n",
    "\n",
    "# y_prob= model.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd0c8e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['target'].reshape(-1, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "05787391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.2778\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 3.0879\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 2.9111\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 2.7500\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 691us/step - loss: 2.5730\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 2.4240\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 2.2614\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.1055\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.9580\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 1.8116\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 333us/step - loss: 1.6829\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.5657\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 1.4525\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.3654\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2999\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.2595\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 1.2214\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.2015\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 1.1868\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.1743\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.1606\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 1.1482\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.1349\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.1220\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.1100\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.0987\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.0874\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.0758\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.0647\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0546\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.0444\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.0343\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.0253\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.0146\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.0051\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.9958\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 664us/step - loss: 0.9866\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 0.9777\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.9688\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.9603\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.9509\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 0.9426\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.9343\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.9258\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9175\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.9092\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.9013\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8936\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8859\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8778\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8706\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8626\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8553\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8479\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8409\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8331\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 333us/step - loss: 0.8261\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8191\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8121\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8052\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7981\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.7913\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.7845\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.7779\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.7714\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7645\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7586\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7517\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7454\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7391\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.7330\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.7266\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.7204\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 0.7143\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7083\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.7023\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6962\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6906\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6846\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6787\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6731\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6675\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6619\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6563\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 0.6508\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6456\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6407\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6348\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6295\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6243\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6191\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6143\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6091\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.6041\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5992\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5941\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5893\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5844\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5803\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5754\n",
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "iris = load_iris()\n",
    "\n",
    "x_feat = iris['data']\n",
    "y_target = iris['target'].reshape(-1, 1)\n",
    "\n",
    "y_onehot = to_categorical(y_target)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_feat, y_onehot, test_size= 0.2)\n",
    "xInput =Input(batch_shape=(None, x_train.shape[1]))  \n",
    "hLayer = Dense(10)(xInput)\n",
    "\n",
    "yOutput = Dense(y_train.shape[1], activation = 'softmax')(hLayer) \n",
    "\n",
    "model = Model(xInput, yOutput)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=100)\n",
    "\n",
    "#평가\n",
    "\n",
    "y_prob = model.predict(x_test)\n",
    "y_pred = np.argmax(y_prob, axis= 1)\n",
    "y_test1 = np.argmax(y_test, axis= 1)\n",
    "\n",
    "\n",
    "\n",
    "acc = (y_test1 == y_pred).mean()\n",
    "print(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "15aac40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 3.1867\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 2.9845\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 2.7870\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 2.5989\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.4051\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2297\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0594\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.8783\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.7028\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5507\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 1.4036\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2592\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 1.1387\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 668us/step - loss: 1.0601\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.9923\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.9472\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.9219\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.9068\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8972\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8866\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 0.8762\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8653\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8544\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 0.8440\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8339\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8237\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8144\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.8057\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7968\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.7882\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7799\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.7716\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 0.7623\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.7547\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7465\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7389\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.7311\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 999us/step - loss: 0.7232\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.7162\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.7087\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7016\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6948\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6881\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6823\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 999us/step - loss: 0.6750\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6687\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6622\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6562\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.6499\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.6442\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.6384\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.6328\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6275\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6222\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6167\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6119\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6076\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6015\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5969\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5918\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5871\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5828\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5782\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5735\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5693\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 0.5658\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5612\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5567\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 0.5527\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5497\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5456\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5413\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5377\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5343\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5310\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5280\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 334us/step - loss: 0.5243\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5207\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5172\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5139\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5112\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5080\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5052\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5030\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.4988\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.4972\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.4932\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4900\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.4883\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.4855\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.4826\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.4796\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.4779\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4746\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.4724\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4698\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 0.4674\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.4654\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.4627\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.4604\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "#One-hot encoding 없이\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "x_feat = iris['data']\n",
    "y_target = iris['target'].reshape(-1, 1)\n",
    "n_class = len(set(iris['target']))\n",
    "\n",
    "# y_onehot = to_categorical(y_target)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_feat, y_target, test_size= 0.2)\n",
    "xInput =Input(batch_shape=(None, x_train.shape[1]))  \n",
    "\n",
    "hLayer = Dense(10)(xInput)\n",
    "# hLayer = Dense(10, activation = 'Relu')(xInput)\n",
    "\n",
    "yOutput = Dense(n_class, activation = 'softmax')(hLayer) \n",
    "\n",
    "model = Model(xInput, yOutput)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=100)\n",
    "\n",
    "#평가\n",
    "\n",
    "y_prob = model.predict(x_test)\n",
    "y_pred = np.argmax(y_prob, axis= 1).reshape(-1, 1)\n",
    "\n",
    "acc = (y_test == y_pred).mean()\n",
    "print(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "02363f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ce79a32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.6696\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 789us/step - loss: 0.6642\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.6603\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.6563\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.6530\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.6493\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 790us/step - loss: 0.6458\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.6431\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.6406\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.6380\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 474us/step - loss: 0.6359\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 790us/step - loss: 0.6334\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.6304\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.6282\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 632us/step - loss: 0.6257\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.6239\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 632us/step - loss: 0.6214\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.6192\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 895us/step - loss: 0.6168\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.6148\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.6124\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.6115\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.6080\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 737us/step - loss: 0.6070\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.6041\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 737us/step - loss: 0.6021\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.6008\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 632us/step - loss: 0.5976\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 541us/step - loss: 0.5961\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 684us/step - loss: 0.5940\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 790us/step - loss: 0.5919\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 632us/step - loss: 0.5889\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5858\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 474us/step - loss: 0.5835\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 632us/step - loss: 0.5810\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 684us/step - loss: 0.5785\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 632us/step - loss: 0.5765\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 684us/step - loss: 0.5746\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 632us/step - loss: 0.5728\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5699\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5687\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5671\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5648\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5623\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 0s 527us/step - loss: 0.5607\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 0s 790us/step - loss: 0.5595\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 0s 609us/step - loss: 0.5567\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 0s 737us/step - loss: 0.5556\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.5544\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5521\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 0s 895us/step - loss: 0.5520\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 0s 527us/step - loss: 0.5497\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.5477\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5464\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.5447\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 0s 842us/step - loss: 0.5433\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 0s 684us/step - loss: 0.5416\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 0s 632us/step - loss: 0.5406\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 0s 684us/step - loss: 0.5396\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 0s 684us/step - loss: 0.5387\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5369\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 0s 790us/step - loss: 0.5350\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 0s 474us/step - loss: 0.5339\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5327\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.5315\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 0s 474us/step - loss: 0.5313\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5291\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 0s 575us/step - loss: 0.5282\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 0s 527us/step - loss: 0.5276\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 0s 895us/step - loss: 0.5267\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.5250\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 0s 632us/step - loss: 0.5238\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 0s 474us/step - loss: 0.5234\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 0s 474us/step - loss: 0.5217\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 0s 737us/step - loss: 0.5210\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5200\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.5187\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 0s 474us/step - loss: 0.5181\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 0s 474us/step - loss: 0.5170\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.5173\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.5149\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 0s 842us/step - loss: 0.5151\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 0s 474us/step - loss: 0.5130\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5121\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5112\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5101\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5097\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5083\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.5083\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 0s 790us/step - loss: 0.5071\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5058\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 0s 534us/step - loss: 0.5053\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 0s 474us/step - loss: 0.5059\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 0s 526us/step - loss: 0.5040\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 0s 632us/step - loss: 0.5040\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.5016\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 0s 474us/step - loss: 0.5029\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 0s 842us/step - loss: 0.5011\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 474us/step - loss: 0.5000\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 0s 579us/step - loss: 0.4994\n",
      "0.7857142857142857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "DATA_PATH = 'C:\\\\Users\\\\배진우\\\\Documents\\\\multiCampus_TA\\\\python_data\\\\'\n",
    "diabetes = pd.read_csv(DATA_PATH + 'diabetes.csv')\n",
    "\n",
    "x_feat = np.array(diabetes.drop('Outcome', axis=1))\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "x_feat = scaler.fit_transform(x_feat)\n",
    "\n",
    "y_target = diabetes['Outcome'].values.reshape(-1, 1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_feat, y_target, test_size= 0.2)\n",
    "xInput =Input(batch_shape=(None, x_train.shape[1]))  \n",
    "\n",
    "# hLayer = Dense(10)(xInput)\n",
    "hLayer = Dense(10, activation = 'relu')(xInput)\n",
    "# hLayer = Dense(10, activation = 'relu')(hLayer)\n",
    "# hLayer = Dense(10, activation = 'relu')(hLayer)\n",
    "\n",
    "yOutput = Dense(y_train.shape[1], activation = 'sigmoid')(hLayer) \n",
    "\n",
    "model = Model(xInput, yOutput)\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'adam')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=100)\n",
    "\n",
    "#평가\n",
    "\n",
    "y_prob =model.predict(x_test) \n",
    "\n",
    "y_pred =(y_prob > 0.5).astype('int8')\n",
    "\n",
    "# acc= (y_test == y_pred).sum()/y_train.shape[0]\n",
    "acc= (y_test == y_pred).mean()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e881c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
